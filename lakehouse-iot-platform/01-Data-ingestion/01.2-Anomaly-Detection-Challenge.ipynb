{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00731703",
   "metadata": {},
   "source": [
    "# üîç Progressive Pipeline Challenge: From Data Quality to Medallion Architecture\n",
    "\n",
    "**Welcome to the advanced pipeline engineering challenge!** Now that you've seen how the basic pipeline works, it's time to extend it with professional data engineering patterns.\n",
    "\n",
    "## üìã Challenge Overview\n",
    "\n",
    "This challenge has **3 progressive levels** - choose your difficulty or complete all three to master pipeline engineering:\n",
    "\n",
    "| Level | Name | Difficulty | What You'll Build |\n",
    "|-------|------|------------|-------------------|\n",
    "| ü•â | **Data Quality Guardian** | Intermediate | Add data quality checks to existing tables |\n",
    "| ü•à | **Anomaly Detective** | Advanced | Create new anomaly detection table |\n",
    "| ü•á | **Pipeline Architect** | Expert | Build complete Bronze‚ÜíSilver‚ÜíGold architecture |\n",
    "\n",
    "---\n",
    "\n",
    "## üìñ Background Story\n",
    "\n",
    "You're a Data Engineer at the wind turbine operations center. The basic pipeline is running, but your team lead says:\n",
    "\n",
    "> *\"The pipeline works, but we need better data quality, automated anomaly detection, and a proper medallion architecture. Can you enhance it?\"*\n",
    "\n",
    "**Your mission:** Extend the pipeline in `01.1-SDP-Wind-Turbine-SQL.ipynb` to make it production-ready!\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Choose Your Challenge Level\n",
    "\n",
    "**New to pipelines?** ‚Üí Start with Level 1  \n",
    "**Comfortable with SQL?** ‚Üí Jump to Level 2  \n",
    "**Want the full experience?** ‚Üí Complete all 3 levels!\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e94694c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ü•â LEVEL 1: Data Quality Guardian\n",
    "\n",
    "**Difficulty:** Intermediate | **Time:** 30 minutes\n",
    "\n",
    "## üìö What You'll Learn\n",
    "\n",
    "- How to add data quality checks with `CONSTRAINT` and `EXPECT`\n",
    "- Different violation strategies: `DROP ROW`, `FAIL`, `QUARANTINE`\n",
    "- Why data quality matters in production pipelines\n",
    "\n",
    "## üéØ Your Mission\n",
    "\n",
    "The operations team reports that sometimes **bad sensor data** causes false alerts. Your task: Add quality checks to prevent invalid data from flowing through the pipeline.\n",
    "\n",
    "## üìù Task Instructions\n",
    "\n",
    "1. Open the pipeline notebook: `01-Data-ingestion/01.1-SDP-Wind-Turbine-SQL.ipynb`\n",
    "2. Find the `sensor_hourly` table definition\n",
    "3. Add **3-5 CONSTRAINT statements** to validate the data\n",
    "\n",
    "### Quality Rules to Implement\n",
    "\n",
    "Add constraints that check for:\n",
    "\n",
    "‚úÖ **Power values are realistic**\n",
    "- Power should be between 0 and 5000 kW\n",
    "- Action: DROP ROW if violated (bad sensor reading)\n",
    "\n",
    "‚úÖ **Vibration is within safe limits**\n",
    "- Vibration should be between 0 and 2\n",
    "- Action: FAIL if violated (pipeline should stop - critical issue!)\n",
    "\n",
    "‚úÖ **No null turbine IDs**\n",
    "- Turbine ID must not be null\n",
    "- Action: DROP ROW if violated\n",
    "\n",
    "‚úÖ **Temperature is reasonable**\n",
    "- Temperature should be between -30 and 50 degrees\n",
    "- Action: DROP ROW if violated\n",
    "\n",
    "‚úÖ **Your choice!** Think of another quality rule\n",
    "\n",
    "## üí° Code Hints\n",
    "\n",
    "```sql\n",
    "-- In 01.1-SDP-Wind-Turbine-SQL.ipynb, find this table:\n",
    "CREATE OR REFRESH LIVE TABLE sensor_hourly\n",
    "-- Add constraints like this AFTER the table name:\n",
    "(\n",
    "  CONSTRAINT valid_power EXPECT (avg_power > 0 AND avg_power < 5000) ON VIOLATION DROP ROW,\n",
    "  CONSTRAINT valid_vibration EXPECT (avg_vibration BETWEEN 0 AND 2) ON VIOLATION FAIL,\n",
    "  -- Add more constraints here...\n",
    ")\n",
    "AS SELECT ...\n",
    "```\n",
    "\n",
    "### ON VIOLATION Strategies\n",
    "\n",
    "- **DROP ROW:** Silently remove bad rows (for occasional bad readings)\n",
    "- **FAIL:** Stop the pipeline (for critical data issues)\n",
    "- **QUARANTINE:** Move to separate table for investigation\n",
    "\n",
    "## ‚úÖ Validation Steps\n",
    "\n",
    "After adding constraints:\n",
    "\n",
    "1. **Save the pipeline notebook**\n",
    "2. **Update your pipeline** (it will reprocess with new rules)\n",
    "3. **Check pipeline metrics:**\n",
    "   ```sql\n",
    "   -- Run this to see quality metrics:\n",
    "   SELECT * FROM event_log(TABLE(LIVE.sensor_hourly))\n",
    "   WHERE details:flow_progress.data_quality.expectations IS NOT NULL\n",
    "   ```\n",
    "4. **Verify:** Did any rows get dropped? Pipeline should show \"expectations met\"\n",
    "\n",
    "## üìä Success Criteria\n",
    "\n",
    "‚úÖ Added at least 3 CONSTRAINT statements  \n",
    "‚úÖ Used different ON VIOLATION strategies  \n",
    "‚úÖ Pipeline runs successfully  \n",
    "‚úÖ Can see quality metrics in pipeline logs  \n",
    "\n",
    "**Level 1 Complete!** üéâ You've added production-grade data quality checks!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e99f5de",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ü•à LEVEL 2: Anomaly Detective\n",
    "\n",
    "**Difficulty:** Advanced | **Time:** 45 minutes\n",
    "\n",
    "## üìö What You'll Learn\n",
    "\n",
    "- How to create new LIVE TABLEs in a pipeline\n",
    "- Window functions and statistical calculations\n",
    "- Z-score methodology for anomaly detection\n",
    "- Pipeline dependencies with `LIVE.table_name`\n",
    "\n",
    "## üéØ Your Mission\n",
    "\n",
    "Manual anomaly queries are too slow! Your manager wants **automated anomaly detection** that updates in real-time as new sensor data arrives.\n",
    "\n",
    "**Goal:** Create a new table `turbine_anomaly_scores` that automatically calculates anomaly scores for every turbine.\n",
    "\n",
    "## üìù Task Instructions\n",
    "\n",
    "1. Open `01-Data-ingestion/01.1-SDP-Wind-Turbine-SQL.ipynb`\n",
    "2. **Add a completely new table** at the end of the pipeline\n",
    "3. Calculate **Z-scores** for power and vibration sensors\n",
    "4. Classify turbines as NORMAL, WARNING, or CRITICAL\n",
    "\n",
    "## üí° Implementation Guide\n",
    "\n",
    "### Step 1: Create the Table Structure\n",
    "\n",
    "Add this to the end of your pipeline notebook:\n",
    "\n",
    "```sql\n",
    "CREATE OR REFRESH LIVE TABLE turbine_anomaly_scores\n",
    "COMMENT \"Real-time anomaly detection for turbines using Z-score method\"\n",
    "AS\n",
    "-- Your code here\n",
    "```\n",
    "\n",
    "### Step 2: Calculate Fleet Statistics\n",
    "\n",
    "Use a CTE (Common Table Expression) to calculate fleet-wide averages:\n",
    "\n",
    "```sql\n",
    "WITH fleet_stats AS (\n",
    "  SELECT \n",
    "    AVG(avg_power) as fleet_mean_power,\n",
    "    STDDEV(avg_power) as fleet_std_power,\n",
    "    AVG(avg_vibration) as fleet_mean_vibration,\n",
    "    STDDEV(avg_vibration) as fleet_std_vibration\n",
    "  FROM LIVE.sensor_hourly  -- Note: LIVE.table_name for pipeline dependencies!\n",
    ")\n",
    "```\n",
    "\n",
    "### Step 3: Calculate Z-Scores\n",
    "\n",
    "For each turbine, calculate how many standard deviations away from the mean:\n",
    "\n",
    "```sql\n",
    "SELECT\n",
    "  s.turbine_id,\n",
    "  s.window,\n",
    "  s.avg_power,\n",
    "  s.avg_vibration,\n",
    "  -- Z-score formula: (value - mean) / std_dev\n",
    "  (s.avg_power - f.fleet_mean_power) / NULLIF(f.fleet_std_power, 0) as power_z_score,\n",
    "  (s.avg_vibration - f.fleet_mean_vibration) / NULLIF(f.fleet_std_vibration, 0) as vibration_z_score\n",
    "FROM LIVE.sensor_hourly s\n",
    "CROSS JOIN fleet_stats f\n",
    "```\n",
    "\n",
    "### Step 4: Add Anomaly Classification\n",
    "\n",
    "Create a status field based on Z-scores:\n",
    "\n",
    "```sql\n",
    "CASE\n",
    "  WHEN ABS(power_z_score) > 3 OR ABS(vibration_z_score) > 3 THEN 'CRITICAL'\n",
    "  WHEN ABS(power_z_score) > 2 OR ABS(vibration_z_score) > 2 THEN 'WARNING'\n",
    "  ELSE 'NORMAL'\n",
    "END as anomaly_level\n",
    "```\n",
    "\n",
    "## üéì Bonus Challenges (Optional)\n",
    "\n",
    "### Bonus 1: Multi-Sensor Score\n",
    "Combine multiple sensors into one anomaly score:\n",
    "```sql\n",
    "(ABS(power_z_score) + ABS(vibration_z_score) + ABS(temp_z_score)) / 3 as composite_score\n",
    "```\n",
    "\n",
    "### Bonus 2: Business Impact\n",
    "Calculate estimated revenue loss for anomalous turbines:\n",
    "```sql\n",
    "CASE \n",
    "  WHEN avg_power < fleet_mean_power \n",
    "  THEN (fleet_mean_power - avg_power) * 0.05  -- $0.05 per kW\n",
    "  ELSE 0 \n",
    "END as estimated_hourly_revenue_loss\n",
    "```\n",
    "\n",
    "### Bonus 3: Add Constraints\n",
    "Add quality checks to your new table:\n",
    "```sql\n",
    "CREATE OR REFRESH LIVE TABLE turbine_anomaly_scores\n",
    "(\n",
    "  CONSTRAINT has_score EXPECT (power_z_score IS NOT NULL),\n",
    "  CONSTRAINT valid_classification EXPECT (anomaly_level IN ('NORMAL', 'WARNING', 'CRITICAL'))\n",
    ")\n",
    "AS ...\n",
    "```\n",
    "\n",
    "## ‚úÖ Validation Steps\n",
    "\n",
    "After creating the table:\n",
    "\n",
    "1. **Save and update pipeline**\n",
    "2. **Check the new table exists:**\n",
    "   ```sql\n",
    "   %sql\n",
    "   SELECT * FROM main.e2eai_iot_turbine.turbine_anomaly_scores\n",
    "   WHERE anomaly_level != 'NORMAL'\n",
    "   ORDER BY ABS(power_z_score) DESC\n",
    "   LIMIT 10;\n",
    "   ```\n",
    "3. **Verify anomalies found:** Should see turbines with WARNING/CRITICAL status\n",
    "\n",
    "## üìä Success Criteria\n",
    "\n",
    "‚úÖ Created new `turbine_anomaly_scores` LIVE TABLE  \n",
    "‚úÖ Calculated Z-scores for at least 2 sensors  \n",
    "‚úÖ Classified turbines as NORMAL/WARNING/CRITICAL  \n",
    "‚úÖ Pipeline runs successfully with new table  \n",
    "\n",
    "**Level 2 Complete!** üéâ You've built automated anomaly detection!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad4ed78",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üéì Learning Reflection & Next Steps\n",
    "\n",
    "## üìù Challenge Completed! What Did You Learn?\n",
    "\n",
    "Take a moment to reflect:\n",
    "\n",
    "### Technical Skills\n",
    "- ‚úÖ Data quality patterns with CONSTRAINTS\n",
    "- ‚úÖ Statistical anomaly detection (Z-scores)\n",
    "- ‚úÖ Pipeline architecture patterns (Medallion)\n",
    "- ‚úÖ SQL advanced features (CTEs, Window Functions)\n",
    "- ‚úÖ Delta Live Tables syntax\n",
    "\n",
    "### Engineering Principles\n",
    "- ‚úÖ Layered data processing (separation of concerns)\n",
    "- ‚úÖ Data quality at every layer\n",
    "- ‚úÖ Business logic in data layer\n",
    "- ‚úÖ Performance optimization strategies\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Real-World Applications\n",
    "\n",
    "### Automated Monitoring\n",
    "```sql\n",
    "-- Create alert query that runs every hour\n",
    "SELECT \n",
    "  turbine_id,\n",
    "  anomaly_level,\n",
    "  estimated_hourly_revenue_loss\n",
    "FROM main.e2eai_iot_turbine.turbine_gold_anomalies\n",
    "WHERE anomaly_level = 'CRITICAL'\n",
    "  AND hour_window.start >= current_timestamp() - INTERVAL 1 HOUR\n",
    "```\n",
    "\n",
    "### Dashboard Integration\n",
    "- Connect Gold tables to Databricks SQL Dashboard\n",
    "- Visualize: Fleet KPIs, Anomaly trends, Revenue impact\n",
    "- Share with: Operations team, Executives\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Next Steps in Your Learning Journey\n",
    "\n",
    "### Immediate Next Steps\n",
    "1. **Module 02 - Data Governance:** Secure your Gold tables with Unity Catalog\n",
    "2. **Module 03 - BI Dashboards:** Build executive dashboard using Gold tables\n",
    "3. **Module 04 - ML Models:** Train ML model on your anomaly features\n",
    "\n",
    "### Advanced Challenges (Come Back Later!)\n",
    "- Implement **Change Data Capture (CDC)**\n",
    "- Add **schema evolution** handling\n",
    "- Build **streaming anomaly detection**\n",
    "- Implement **SLA monitoring**\n",
    "\n",
    "---\n",
    "\n",
    "## üèÜ Congratulations!\n",
    "\n",
    "You've completed a comprehensive pipeline engineering challenge! You now understand:\n",
    "- ‚úÖ Production data quality patterns\n",
    "- ‚úÖ Real-time anomaly detection systems  \n",
    "- ‚úÖ Medallion Architecture best practices\n",
    "- ‚úÖ Business-driven data engineering\n",
    "\n",
    "**You're ready for real-world data engineering projects!** üöÄ\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Additional Resources\n",
    "\n",
    "### Documentation\n",
    "- [Delta Live Tables Guide](https://docs.databricks.com/delta-live-tables/index.html)\n",
    "- [Medallion Architecture](https://www.databricks.com/glossary/medallion-architecture)\n",
    "- [Data Quality Expectations](https://docs.databricks.com/delta-live-tables/expectations.html)\n",
    "\n",
    "### Learning Paths\n",
    "- **Next Module:** [02-Data-Governance](../02-Data-governance/02-UC-data-governance-security-iot-turbine.ipynb)\n",
    "- **Parallel Learning:** [03-BI-Datawarehousing](../03-BI-data-warehousing/03-BI-Datawarehousing-iot-turbine.ipynb)\n",
    "\n",
    "**Happy Engineering!** üéâ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce60db2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ü•á LEVEL 3: Pipeline Architect\n",
    "\n",
    "**Difficulty:** Expert | **Time:** 60 minutes\n",
    "\n",
    "## üìö What You'll Learn\n",
    "\n",
    "- Medallion Architecture (Bronze ‚Üí Silver ‚Üí Gold)\n",
    "- Layered data processing patterns\n",
    "- Incremental processing strategies\n",
    "- Business metrics calculation\n",
    "\n",
    "## üéØ Your Mission\n",
    "\n",
    "Your team lead is impressed! Now they want you to **refactor the entire pipeline** using the Medallion Architecture pattern - the industry standard for data lakes.\n",
    "\n",
    "**Goal:** Build a 3-layer architecture:\n",
    "- üü§ **Bronze:** Raw data with minimal cleaning\n",
    "- ü•à **Silver:** Business logic and aggregations  \n",
    "- ü•á **Gold:** Analytics-ready tables with business metrics\n",
    "\n",
    "## üìù Architecture Overview\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  BRONZE Layer (Raw + Validated)                        ‚îÇ\n",
    "‚îÇ  - sensor_bronze_clean: Deduplicated raw sensor data   ‚îÇ\n",
    "‚îÇ  - Quality: Structural validation only                 ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                         ‚Üì\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  SILVER Layer (Business Logic)                         ‚îÇ\n",
    "‚îÇ  - sensor_silver_hourly: Hourly aggregations           ‚îÇ\n",
    "‚îÇ  - Quality: Business rule validation                   ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                         ‚Üì\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  GOLD Layer (Analytics)                                ‚îÇ\n",
    "‚îÇ  - turbine_gold_anomalies: Anomaly detection + scores  ‚îÇ\n",
    "‚îÇ  - turbine_gold_kpis: Business metrics & revenue       ‚îÇ\n",
    "‚îÇ  - Quality: Analytics validation                       ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "## üí° Implementation Guide\n",
    "\n",
    "### üü§ BRONZE LAYER: Clean Raw Data\n",
    "\n",
    "**Purpose:** Store raw data with minimal transformations, just remove duplicates and enforce structure.\n",
    "\n",
    "```sql\n",
    "-- Bronze Layer: Deduplicated sensor data\n",
    "CREATE OR REFRESH LIVE TABLE sensor_bronze_clean\n",
    "(\n",
    "  CONSTRAINT valid_timestamp EXPECT (timestamp IS NOT NULL) ON VIOLATION DROP ROW,\n",
    "  CONSTRAINT valid_turbine_id EXPECT (turbine_id IS NOT NULL) ON VIOLATION DROP ROW\n",
    ")\n",
    "COMMENT \"Bronze: Cleaned and deduplicated raw sensor readings\"\n",
    "AS \n",
    "SELECT DISTINCT\n",
    "  turbine_id,\n",
    "  timestamp,\n",
    "  AN,\n",
    "  AVALUES,\n",
    "  SPEED,\n",
    "  TORQUE,\n",
    "  FORCE\n",
    "FROM LIVE.sensor_bronze\n",
    "WHERE turbine_id IS NOT NULL;\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ü•à SILVER LAYER: Business Logic\n",
    "\n",
    "**Purpose:** Apply business transformations, aggregations, and enrichments.\n",
    "\n",
    "```sql\n",
    "CREATE OR REFRESH LIVE TABLE sensor_silver_hourly\n",
    "(\n",
    "  CONSTRAINT power_reasonable EXPECT (avg_power BETWEEN 0 AND 5000) ON VIOLATION DROP ROW,\n",
    "  CONSTRAINT vibration_safe EXPECT (avg_vibration BETWEEN 0 AND 2) ON VIOLATION FAIL\n",
    ")\n",
    "COMMENT \"Silver: Hourly sensor aggregations with business rules\"\n",
    "AS\n",
    "SELECT\n",
    "  turbine_id,\n",
    "  window(timestamp, \"1 hour\") as hour_window,\n",
    "  AVG(AVALUES) as avg_power,\n",
    "  MAX(AVALUES) as max_power,\n",
    "  STDDEV(AVALUES) as power_variability,\n",
    "  AVG(SPEED) as avg_vibration,\n",
    "  AVG(TORQUE) as avg_temperature,\n",
    "  COUNT(*) as reading_count\n",
    "FROM LIVE.sensor_bronze_clean\n",
    "GROUP BY turbine_id, window(timestamp, \"1 hour\");\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ü•á GOLD LAYER: Analytics-Ready\n",
    "\n",
    "**Purpose:** Business-ready tables with KPIs, anomaly scores, and revenue metrics.\n",
    "\n",
    "```sql\n",
    "CREATE OR REFRESH LIVE TABLE turbine_gold_anomalies\n",
    "(\n",
    "  CONSTRAINT has_anomaly_score EXPECT (power_z_score IS NOT NULL),\n",
    "  CONSTRAINT valid_level EXPECT (anomaly_level IN ('NORMAL', 'WARNING', 'CRITICAL'))\n",
    ")\n",
    "COMMENT \"Gold: Real-time anomaly detection with business impact\"\n",
    "AS\n",
    "WITH fleet_stats AS (\n",
    "  SELECT \n",
    "    AVG(avg_power) as fleet_mean_power,\n",
    "    STDDEV(avg_power) as fleet_std_power,\n",
    "    AVG(avg_vibration) as fleet_mean_vibration,\n",
    "    STDDEV(avg_vibration) as fleet_std_vibration\n",
    "  FROM LIVE.sensor_silver_hourly\n",
    ")\n",
    "SELECT\n",
    "  s.turbine_id,\n",
    "  s.hour_window,\n",
    "  s.avg_power,\n",
    "  s.avg_vibration,\n",
    "  \n",
    "  -- Z-scores\n",
    "  (s.avg_power - f.fleet_mean_power) / NULLIF(f.fleet_std_power, 0) as power_z_score,\n",
    "  (s.avg_vibration - f.fleet_mean_vibration) / NULLIF(f.fleet_std_vibration, 0) as vibration_z_score,\n",
    "  \n",
    "  -- Anomaly classification\n",
    "  CASE\n",
    "    WHEN ABS((s.avg_power - f.fleet_mean_power) / NULLIF(f.fleet_std_power, 0)) > 3 THEN 'CRITICAL'\n",
    "    WHEN ABS((s.avg_power - f.fleet_mean_power) / NULLIF(f.fleet_std_power, 0)) > 2 THEN 'WARNING'\n",
    "    ELSE 'NORMAL'\n",
    "  END as anomaly_level,\n",
    "  \n",
    "  -- Business impact: Revenue loss estimation\n",
    "  CASE \n",
    "    WHEN s.avg_power < f.fleet_mean_power \n",
    "    THEN (f.fleet_mean_power - s.avg_power) * 0.05\n",
    "    ELSE 0 \n",
    "  END as estimated_hourly_revenue_loss\n",
    "  \n",
    "FROM LIVE.sensor_silver_hourly s\n",
    "CROSS JOIN fleet_stats f;\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Validation Steps\n",
    "\n",
    "1. **Save pipeline with all 3 layers**\n",
    "2. **Update pipeline** (may take 10-15 min)\n",
    "3. **Verify all tables exist:**\n",
    "   ```sql\n",
    "   SHOW TABLES IN main.e2eai_iot_turbine;\n",
    "   ```\n",
    "4. **Query Gold tables:**\n",
    "   ```sql\n",
    "   SELECT * FROM main.e2eai_iot_turbine.turbine_gold_anomalies\n",
    "   WHERE anomaly_level = 'CRITICAL'\n",
    "   ORDER BY estimated_hourly_revenue_loss DESC;\n",
    "   ```\n",
    "\n",
    "## üìä Success Criteria\n",
    "\n",
    "‚úÖ Created Bronze layer (cleaned data)  \n",
    "‚úÖ Created Silver layer (aggregations)  \n",
    "‚úÖ Created Gold layer (business metrics)  \n",
    "‚úÖ All tables have appropriate constraints  \n",
    "‚úÖ Can query business KPIs from Gold layer  \n",
    "\n",
    "**Level 3 Complete!** üèÜ You've built a production-grade Medallion Architecture!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9cd1990",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: See what tables the pipeline created\n",
    "%sql\n",
    "SHOW TABLES IN main.e2eai_iot_turbine;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a627f97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Explore the sensor_hourly table structure\n",
    "%sql\n",
    "SELECT * FROM main.e2eai_iot_turbine.sensor_hourly LIMIT 10;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3df6e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Check how many turbines and time range\n",
    "%sql\n",
    "SELECT \n",
    "  COUNT(DISTINCT turbine_id) as total_turbines,\n",
    "  MIN(window.start) as earliest_reading,\n",
    "  MAX(window.end) as latest_reading,\n",
    "  COUNT(*) as total_records\n",
    "FROM main.e2eai_iot_turbine.sensor_hourly;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff05a83",
   "metadata": {},
   "source": [
    "## üìä Analysis Ideas\n",
    "\n",
    "Here are some approaches you could try (pick one or combine several!):\n",
    "\n",
    "### Approach 1: Statistical Outliers\n",
    "- Calculate mean and standard deviation for each sensor\n",
    "- Find turbines with values beyond 2-3 standard deviations\n",
    "- Use Z-scores or IQR (Interquartile Range) method\n",
    "\n",
    "### Approach 2: Comparative Analysis\n",
    "- Compare each turbine against fleet averages\n",
    "- Identify turbines in the top/bottom 5% for key metrics\n",
    "- Look for unusual combinations of sensor values\n",
    "\n",
    "### Approach 3: Pattern Detection\n",
    "- Identify sudden drops in power output\n",
    "- Find turbines with high vibration + low power\n",
    "- Detect sensors with abnormally high variability\n",
    "\n",
    "### Approach 4: Multi-Sensor Correlation\n",
    "- Normal turbines: high wind speed ‚Üí high power\n",
    "- Anomalous: high wind speed but low power (potential failure)\n",
    "- Check for sensor combinations that don't make physical sense"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea1441f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üíª Your Analysis Code\n",
    "\n",
    "**Instructions:** Write your anomaly detection code below. You can use SQL, Python with Pandas, or PySpark - whatever you're comfortable with!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571a5c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE - Step 1: Exploratory Analysis\n",
    "# Ideas:\n",
    "# - Look at distributions of key metrics (power, vibration, temperature)\n",
    "# - Calculate summary statistics per turbine\n",
    "# - Identify interesting patterns or outliers\n",
    "\n",
    "# Example starter code:\n",
    "# %sql\n",
    "# SELECT \n",
    "#   turbine_id,\n",
    "#   AVG(avg_power) as mean_power,\n",
    "#   STDDEV(avg_power) as stddev_power,\n",
    "#   AVG(avg_vibration) as mean_vibration,\n",
    "#   COUNT(*) as num_readings\n",
    "# FROM main.e2eai_iot_turbine.sensor_hourly\n",
    "# GROUP BY turbine_id\n",
    "# ORDER BY mean_power DESC;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d421ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE - Step 2: Anomaly Detection Logic\n",
    "# Ideas:\n",
    "# - Calculate Z-scores for key sensors\n",
    "# - Find outliers using statistical thresholds\n",
    "# - Compare against fleet averages\n",
    "# - Look for sensor correlation issues\n",
    "\n",
    "# Example: Finding turbines with unusual power output\n",
    "# %sql\n",
    "# WITH fleet_stats AS (\n",
    "#   SELECT \n",
    "#     AVG(avg_power) as fleet_mean,\n",
    "#     STDDEV(avg_power) as fleet_std\n",
    "#   FROM main.e2eai_iot_turbine.sensor_hourly\n",
    "# )\n",
    "# SELECT \n",
    "#   s.turbine_id,\n",
    "#   AVG(s.avg_power) as turbine_mean,\n",
    "#   (AVG(s.avg_power) - f.fleet_mean) / f.fleet_std as z_score\n",
    "# FROM main.e2eai_iot_turbine.sensor_hourly s\n",
    "# CROSS JOIN fleet_stats f\n",
    "# GROUP BY s.turbine_id, f.fleet_mean, f.fleet_std\n",
    "# HAVING ABS((AVG(s.avg_power) - f.fleet_mean) / f.fleet_std) > 2;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c150d9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE - Step 3: Severity Ranking\n",
    "# Ideas:\n",
    "# - Assign severity scores based on multiple factors\n",
    "# - Consider: deviation magnitude, number of anomalous sensors, business impact\n",
    "# - Rank turbines by priority for inspection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0584ee0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìù Document Your Findings\n",
    "\n",
    "**Fill in your analysis results below:**\n",
    "\n",
    "### Turbines Flagged as Anomalous\n",
    "\n",
    "1. **Turbine ID:** `___`\n",
    "   - **Reason:** ___ (e.g., \"Power output 3.5 std dev below fleet average\")\n",
    "   - **Affected Sensors:** ___ (e.g., avg_power, avg_vibration)\n",
    "   - **Severity:** ___ (Critical / High / Medium / Low)\n",
    "\n",
    "2. **Turbine ID:** `___`\n",
    "   - **Reason:** ___\n",
    "   - **Affected Sensors:** ___\n",
    "   - **Severity:** ___\n",
    "\n",
    "3. **Turbine ID:** `___`\n",
    "   - **Reason:** ___\n",
    "   - **Affected Sensors:** ___\n",
    "   - **Severity:** ___\n",
    "\n",
    "*(Add more as needed)*\n",
    "\n",
    "---\n",
    "\n",
    "### Your Methodology\n",
    "\n",
    "**Statistical Approach Used:**\n",
    "- ___ (e.g., \"Z-score method with threshold of 2.5 standard deviations\")\n",
    "\n",
    "**Thresholds Defined:**\n",
    "- ___ (e.g., \"Power: <500kW flagged as critical, Vibration: >0.8 flagged\")\n",
    "\n",
    "**Prioritization Logic:**\n",
    "- ___ (e.g., \"Ranked by: 1) safety risk, 2) revenue impact, 3) repair cost\")\n",
    "\n",
    "---\n",
    "\n",
    "### Recommendation for Operations Team\n",
    "\n",
    "**Immediate Actions:**\n",
    "1. Inspect Turbine `___` first because: ___\n",
    "2. Schedule maintenance for Turbine `___` within: ___ days\n",
    "3. Monitor Turbine `___` closely for: ___ (specific sensor)\n",
    "\n",
    "**Root Cause Hypotheses:**\n",
    "- ___ (e.g., \"Turbine 42 may have bearing failure based on high vibration + low power\")\n",
    "\n",
    "**Estimated Business Impact:**\n",
    "- ___ (e.g., \"3 turbines down = ~1.5MW lost capacity = $X per day revenue loss\")\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365cd01b",
   "metadata": {},
   "source": [
    "## üéì Learning Reflection\n",
    "\n",
    "Answer these questions after completing the challenge:\n",
    "\n",
    "1. **What was the hardest part of this analysis?**\n",
    "   - ___\n",
    "\n",
    "2. **What would you do differently with more time?**\n",
    "   - ___\n",
    "\n",
    "3. **What additional data would help improve your analysis?**\n",
    "   - ___\n",
    "\n",
    "4. **How would you automate this in production?**\n",
    "   - ___ (e.g., \"Create scheduled job to run analysis daily and send alerts\")\n",
    "\n",
    "---\n",
    "\n",
    "## üí° Real-World Tips\n",
    "\n",
    "In a production environment, you would:\n",
    "\n",
    "‚úÖ **Create an automated monitoring dashboard** with real-time alerts  \n",
    "‚úÖ **Set up scheduled jobs** to run anomaly detection hourly/daily  \n",
    "‚úÖ **Integrate with ticketing systems** to auto-create maintenance work orders  \n",
    "‚úÖ **Track historical anomalies** to improve detection algorithms over time  \n",
    "‚úÖ **Build feedback loops** where field technician findings improve the model  \n",
    "\n",
    "This is exactly the kind of work Data Engineers do every day!\n",
    "\n",
    "---\n",
    "\n",
    "## üèÜ Next Steps\n",
    "\n",
    "Once you've completed this challenge:\n",
    "\n",
    "1. **Share your findings** with your learning group (if applicable)\n",
    "2. **Compare approaches** - Did others use different statistical methods?\n",
    "3. **Move to Module 02** - Data Governance to secure these insights\n",
    "4. **Later**: Build an ML model in Module 04 to automate this detection!\n",
    "\n",
    "**Great job on your first data engineering challenge!** üéâ"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
