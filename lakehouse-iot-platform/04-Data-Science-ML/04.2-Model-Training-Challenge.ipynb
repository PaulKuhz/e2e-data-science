{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e98a5fe",
   "metadata": {},
   "source": [
    "# ü§ñ ML Model Training Challenge\n",
    "\n",
    "**Welcome to the Machine Learning Challenge!** Now that you've explored and prepared your data in **04.1**, it's time to **build, train, and optimize predictive models**.\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Your Mission\n",
    "\n",
    "Build a **predictive maintenance model** that can:\n",
    "1. **Predict which sensor will fail** before it happens ‚ùå\n",
    "2. **Compare different ML algorithms** (Logistic Regression vs XGBoost)\n",
    "3. **Optimize hyperparameters** to maximize accuracy\n",
    "4. **Track experiments** using MLflow\n",
    "5. **Register the best model** in Unity Catalog for production deployment\n",
    "\n",
    "---\n",
    "\n",
    "## üìä The Challenge\n",
    "\n",
    "**Scenario:** Your energy company loses **millions** every time a wind turbine breaks down unexpectedly. Your ML model must:\n",
    "- **Classify sensor status:** `'ok'`, `'sensor_B'`, `'sensor_E'`, `'sensor_F'` (multi-class classification)\n",
    "- **Achieve >85% accuracy** to be production-worthy\n",
    "- **Be explainable** - which features matter most?\n",
    "\n",
    "---\n",
    "\n",
    "## üèÜ Challenge Levels\n",
    "\n",
    "### **Level 1: Basic Model Training (30 min)**\n",
    "- Load training data from `turbine_hourly_features`\n",
    "- Train Logistic Regression model\n",
    "- Log metrics to MLflow\n",
    "- Achieve baseline accuracy\n",
    "\n",
    "### **Level 2: Advanced Model Comparison (30 min)**\n",
    "- Train XGBoost Classifier\n",
    "- Compare multiple models\n",
    "- Select best performing model\n",
    "\n",
    "### **Level 3: Hyperparameter Optimization (45 min)**\n",
    "- Use Optuna for automated hyperparameter tuning\n",
    "- Run 10+ optimization trials\n",
    "- Register best model to Unity Catalog with @prod alias\n",
    "\n",
    "---\n",
    "\n",
    "## üìù What You'll Learn\n",
    "\n",
    "‚úÖ **Multi-class classification** - Predict multiple failure types  \n",
    "‚úÖ **MLflow experiment tracking** - Track every model, parameter, and metric  \n",
    "‚úÖ **Model comparison** - Logistic Regression vs XGBoost  \n",
    "‚úÖ **Hyperparameter tuning** - Optuna for automated optimization  \n",
    "‚úÖ **Unity Catalog model registry** - Production-grade model management  \n",
    "‚úÖ **Model signatures** - Ensure schema compatibility  \n",
    "\n",
    "---\n",
    "\n",
    "Let's start! üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37065354",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üì¶ Setup: Install Libraries\n",
    "\n",
    "**What you need to do:**\n",
    "Install required packages:\n",
    "- `mlflow` - Experiment tracking & model registry\n",
    "- `optuna` - Hyperparameter optimization\n",
    "- `xgboost` - Gradient boosting library\n",
    "\n",
    "**üí° Note:** After installation, Python kernel will restart automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a85e605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN THIS CELL - No changes needed\n",
    "%pip install --quiet databricks-sdk==0.40.0 mlflow==2.22.0 optuna optuna-integration[mlflow] xgboost\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f891036c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN THIS CELL - No changes needed\n",
    "%run ../_resources/00-setup $reset_all_data=false"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9a2e94",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üì• Task 1: Import Libraries & Configure MLflow\n",
    "\n",
    "**What you need to do:**\n",
    "1. Import necessary libraries for ML training\n",
    "2. Configure MLflow to use Unity Catalog as model registry\n",
    "\n",
    "**Libraries you'll need:**\n",
    "- **sklearn:** `train_test_split`, `StandardScaler`, `LogisticRegression`\n",
    "- **sklearn.metrics:** `accuracy_score`, `precision_score`, `recall_score`, `f1_score`\n",
    "- **xgboost:** `XGBClassifier`\n",
    "- **mlflow:** Model tracking and registry\n",
    "- **optuna:** Hyperparameter optimization\n",
    "\n",
    "**üí° Hint:** Use `mlflow.set_registry_uri('databricks-uc')` to enable Unity Catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4e3fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE: Import libraries from sklearn\n",
    "# Hint: from sklearn.model_selection import train_test_split\n",
    "#       from sklearn.preprocessing import StandardScaler\n",
    "#       from sklearn.linear_model import LogisticRegression\n",
    "#       from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e1997c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE: Import XGBoost\n",
    "# Hint: from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72e7e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE: Import MLflow, Optuna, and other utilities\n",
    "# Hint: import mlflow\n",
    "#       from mlflow.models import infer_signature\n",
    "#       from mlflow import MlflowClient\n",
    "#       import optuna\n",
    "#       from optuna.integration.mlflow import MLflowCallback\n",
    "#       import numpy as np\n",
    "#       import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991ecb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE: Configure MLflow to use Unity Catalog\n",
    "# Hint: mlflow.set_registry_uri('databricks-uc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed82475",
   "metadata": {},
   "source": [
    "### ‚úÖ Success Criteria:\n",
    "- All libraries imported without errors\n",
    "- MLflow configured to use Unity Catalog registry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7773634e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìä Task 2: Load & Prepare Training Data\n",
    "\n",
    "**What you need to do:**\n",
    "1. Load `turbine_hourly_features` table (created in 04.1)\n",
    "2. Drop `turbine_id` column (not a predictive feature)\n",
    "3. Separate features (X) and target (y)\n",
    "4. Encode target labels as integers\n",
    "5. Split into train/test sets (80/20 split)\n",
    "\n",
    "**Feature columns (X):**\n",
    "- `avg_energy`\n",
    "- `std_sensor_A`, `std_sensor_B`, `std_sensor_C`, `std_sensor_D`, `std_sensor_E`, `std_sensor_F`\n",
    "\n",
    "**Target column (y):**\n",
    "- `abnormal_sensor` (multi-class: 'ok', 'sensor_B', 'sensor_E', 'sensor_F')\n",
    "\n",
    "**üí° Hints:**\n",
    "- Use `spark.table()` to load data\n",
    "- Convert to pandas with `.toPandas()`\n",
    "- Encode labels: `pd.factorize(y)` converts strings to integers\n",
    "- Use `train_test_split()` with `test_size=0.2` and `random_state=42`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6bff466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE: Define table name and load data\n",
    "# Hint: features_table_name = f'{catalog}.{db}.turbine_hourly_features'\n",
    "#       training_dataset = spark.table(features_table_name).drop('turbine_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c979de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE: Prepare features (X) and target (y)\n",
    "# Hint: X = training_dataset.toPandas()[['avg_energy', 'std_sensor_A', ...]]\n",
    "#       y = training_dataset.toPandas()['abnormal_sensor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b412e2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE: Encode target labels\n",
    "# Hint: y_encoded = pd.factorize(y)[0]\n",
    "# This converts 'ok', 'sensor_B', etc. to 0, 1, 2, 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a2db78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE: Split into train and test sets\n",
    "# Hint: X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01116c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE: Print dataset shapes to verify\n",
    "# How many samples in train vs test?\n",
    "# Hint: print(f\"Train size: {X_train.shape}, Test size: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71c10d8",
   "metadata": {},
   "source": [
    "### ‚úÖ Success Criteria:\n",
    "- Training set: ~80% of data\n",
    "- Test set: ~20% of data\n",
    "- Features (X) contain 7 columns\n",
    "- Target (y) encoded as integers (0, 1, 2, ...)\n",
    "- No missing values in X_train or X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904f5fda",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìù Task 3: Create Model Signature\n",
    "\n",
    "**What you need to do:**\n",
    "1. Create MLflow model signature (defines input/output schema)\n",
    "2. Create input example (for model testing)\n",
    "\n",
    "**üí° Why?** Unity Catalog requires models to have signatures for schema validation and governance.\n",
    "\n",
    "**Hints:**\n",
    "- Use `mlflow.models.infer_signature(X_train, y_train)`\n",
    "- Input example: `X_train.iloc[[0]]` (first row with column names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf78f0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE: Create model signature\n",
    "# Hint: signature = infer_signature(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb8aa6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE: Display signature\n",
    "# Verify it shows input features and output type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ae07fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE: Create input example\n",
    "# Hint: input_example = X_train.iloc[[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2718d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE: Display input example\n",
    "# Verify it shows all feature columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56195614",
   "metadata": {},
   "source": [
    "### ‚úÖ Success Criteria:\n",
    "- Signature shows 7 input features (all sensors + avg_energy)\n",
    "- Signature shows output type (integer for class)\n",
    "- Input example contains 1 row with all features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca3d7d1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üéØ LEVEL 1: Build Logistic Regression Model (30 min)\n",
    "\n",
    "**Goal:** Train a baseline model and log it to MLflow\n",
    "\n",
    "---\n",
    "\n",
    "## üìä Task 4: Train Logistic Regression\n",
    "\n",
    "**What you need to do:**\n",
    "1. Start an MLflow run (with run_name=\"Logistic Regression Run\")\n",
    "2. Define Logistic Regression model with `max_iter=200`\n",
    "3. Fit model on training data\n",
    "4. Predict on test data\n",
    "5. Calculate metrics: accuracy, precision, recall, f1_score\n",
    "6. Log parameters, metrics, and model to MLflow\n",
    "\n",
    "**üí° Important:**\n",
    "- Use `average=\"macro\"` for multi-class metrics\n",
    "- Log model with signature and input_example\n",
    "\n",
    "**Hints:**\n",
    "```python\n",
    "with mlflow.start_run(run_name=\"...\"):\n",
    "    # 1. Define model\n",
    "    # 2. Train model\n",
    "    # 3. Make predictions\n",
    "    # 4. Calculate metrics\n",
    "    # 5. Log everything to MLflow\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55dbd03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE: Train Logistic Regression model\n",
    "# Remember to wrap everything in mlflow.start_run()\n",
    "\n",
    "# Step 1: Start MLflow run\n",
    "\n",
    "# Step 2: Define model (LogisticRegression with max_iter=200)\n",
    "\n",
    "# Step 3: Train model (.fit)\n",
    "\n",
    "# Step 4: Make predictions (.predict on X_test)\n",
    "\n",
    "# Step 5: Calculate metrics\n",
    "# Hint: accuracy_score(y_test, predictions)\n",
    "#       precision_score(y_test, predictions, average=\"macro\")\n",
    "#       recall_score(y_test, predictions, average=\"macro\")\n",
    "#       f1_score(y_test, predictions, average=\"macro\")\n",
    "\n",
    "# Step 6: Log to MLflow\n",
    "# mlflow.set_tag(\"model_family\", \"LogisticRegression\")\n",
    "# mlflow.log_param(\"max_iter\", 200)\n",
    "# mlflow.log_metric(\"accuracy\", acc)\n",
    "# mlflow.log_metric(\"precision\", precision)\n",
    "# mlflow.log_metric(\"recall\", recall)\n",
    "# mlflow.log_metric(\"f1_score\", f1)\n",
    "\n",
    "# Step 7: Log model\n",
    "# mlflow.sklearn.log_model(\n",
    "#     lr_model,\n",
    "#     artifact_path=\"logreg_model\",\n",
    "#     input_example=input_example,\n",
    "#     signature=signature\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd03a0f",
   "metadata": {},
   "source": [
    "### ü§î Reflection Questions:\n",
    "\n",
    "**1. What accuracy did you achieve?**\n",
    "```\n",
    "[Your answer: e.g., 87.5%]\n",
    "```\n",
    "\n",
    "**2. Is the model production-ready (>85% accuracy)?**\n",
    "```\n",
    "[Your answer: Yes/No]\n",
    "```\n",
    "\n",
    "**3. Which metric matters most for predictive maintenance?**\n",
    "- Accuracy: Overall correctness\n",
    "- Precision: Avoid false alarms\n",
    "- Recall: Don't miss failures\n",
    "- F1: Balance of precision and recall\n",
    "```\n",
    "[Your answer]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e538f236",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üî• LEVEL 2: Build XGBoost Model (30 min)\n",
    "\n",
    "**Goal:** Train a more advanced model and compare with baseline\n",
    "\n",
    "---\n",
    "\n",
    "## üìä Task 5: Train XGBoost Classifier\n",
    "\n",
    "**What you need to do:**\n",
    "1. Start a new MLflow run (with run_name=\"XGBoost Run\")\n",
    "2. Define XGBoost model with:\n",
    "   - `objective=\"multi:softprob\"` (multi-class classification)\n",
    "   - `num_class=3` (number of classes)\n",
    "   - `max_depth=3`\n",
    "   - `n_estimators=100`\n",
    "   - `learning_rate=0.1`\n",
    "   - `eval_metric=\"mlogloss\"`\n",
    "3. Train, predict, evaluate, and log to MLflow\n",
    "\n",
    "**üí° Hint:** Structure is similar to Logistic Regression, but use `XGBClassifier` and `mlflow.xgboost.log_model()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16abd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE: Train XGBoost model\n",
    "\n",
    "# Step 1: Start MLflow run\n",
    "\n",
    "# Step 2: Define XGBoost model\n",
    "# Hint: xgb_model = XGBClassifier(\n",
    "#           objective=\"multi:softprob\",\n",
    "#           num_class=3,\n",
    "#           max_depth=3,\n",
    "#           n_estimators=100,\n",
    "#           learning_rate=0.1,\n",
    "#           use_label_encoder=False,\n",
    "#           eval_metric=\"mlogloss\"\n",
    "#       )\n",
    "\n",
    "# Step 3: Train model\n",
    "\n",
    "# Step 4: Make predictions\n",
    "\n",
    "# Step 5: Calculate metrics (same as before)\n",
    "\n",
    "# Step 6: Log parameters\n",
    "# mlflow.set_tag(\"model_family\", \"XGBOOST\")\n",
    "# mlflow.log_param(\"max_depth\", 3)\n",
    "# mlflow.log_param(\"n_estimators\", 100)\n",
    "# mlflow.log_param(\"learning_rate\", 0.1)\n",
    "\n",
    "# Step 7: Log metrics\n",
    "\n",
    "# Step 8: Log model\n",
    "# mlflow.xgboost.log_model(\n",
    "#     xgb_model,\n",
    "#     artifact_path=\"xgb_model\",\n",
    "#     input_example=input_example,\n",
    "#     signature=signature\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10988a55",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìà Task 6: Compare Models\n",
    "\n",
    "**What you need to do:**\n",
    "1. Search all MLflow runs\n",
    "2. Sort by accuracy (descending)\n",
    "3. Identify best performing model\n",
    "4. Compare Logistic Regression vs XGBoost\n",
    "\n",
    "**üí° Hints:**\n",
    "- Use `mlflow.search_runs()` with `order_by=['metrics.accuracy DESC']`\n",
    "- Display top 5 runs\n",
    "- Check `tags.model_family` column to identify model type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188ef1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE: Search and display all runs\n",
    "# Hint: mlflow.search_runs(order_by=['metrics.accuracy DESC','start_time DESC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb9da85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE: Get best run details\n",
    "# Hint: best_run = mlflow.search_runs(order_by=['metrics.accuracy DESC']).iloc[0]\n",
    "#       print(f\"Best accuracy: {best_run['metrics.accuracy']}\")\n",
    "#       print(f\"Best model type: {best_run['tags.model_family']}\")\n",
    "#       best_model_run_id = best_run['run_id']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85491863",
   "metadata": {},
   "source": [
    "### ü§î Reflection Questions:\n",
    "\n",
    "**1. Which model performed better: Logistic Regression or XGBoost?**\n",
    "```\n",
    "[Your answer]\n",
    "```\n",
    "\n",
    "**2. By how much did accuracy improve?**\n",
    "```\n",
    "[Your answer: e.g., +3.2%]\n",
    "```\n",
    "\n",
    "**3. Why might XGBoost perform better for this problem?**\n",
    "```\n",
    "[Your answer: Think about non-linear relationships, feature interactions]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39fa28ca",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üöÄ LEVEL 3: Hyperparameter Optimization (45 min)\n",
    "\n",
    "**Goal:** Use Optuna to find optimal XGBoost parameters\n",
    "\n",
    "---\n",
    "\n",
    "## üîß Task 7: Set Up Optuna Optimization\n",
    "\n",
    "**What you need to do:**\n",
    "1. Get current experiment ID from MLflow\n",
    "2. Create Optuna MLflow callback\n",
    "3. Define objective function for hyperparameter tuning\n",
    "4. Run optimization trials\n",
    "\n",
    "**Hyperparameters to tune:**\n",
    "- `num_class`: 2-10\n",
    "- `max_depth`: 2-15\n",
    "- `n_estimators`: 10-100\n",
    "\n",
    "**üí° How Optuna works:**\n",
    "- Tries different parameter combinations\n",
    "- Learns from previous trials\n",
    "- Finds optimal parameters to maximize accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc94af58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE: Get current experiment ID\n",
    "# This finds the MLflow experiment associated with this notebook\n",
    "\n",
    "# Hint:\n",
    "# client = MlflowClient()\n",
    "# experiments = client.search_experiments()\n",
    "# notebook_name = dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get().split(\"/\")[-1]\n",
    "\n",
    "# Then loop through experiments to find matching name\n",
    "# Store experiment_id for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b1726a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE: Create Optuna MLflow callback\n",
    "# This automatically logs each Optuna trial to MLflow\n",
    "\n",
    "# Hint:\n",
    "# mlflow_callback = MLflowCallback(\n",
    "#     tracking_uri='databricks',\n",
    "#     metric_name=\"accuracy\",\n",
    "#     create_experiment=False,\n",
    "#     mlflow_kwargs={\"nested\": True},\n",
    "#     tag_trial_user_attrs=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55ba13b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéØ Task 8: Define Objective Function\n",
    "\n",
    "**What you need to do:**\n",
    "Create a function that:\n",
    "1. Receives hyperparameter suggestions from Optuna\n",
    "2. Trains XGBoost model with those parameters\n",
    "3. Evaluates model on test set\n",
    "4. Logs everything to MLflow\n",
    "5. Returns accuracy (optimization target)\n",
    "\n",
    "**üí° Structure:**\n",
    "```python\n",
    "def objective(trial):\n",
    "    with mlflow.start_run(nested=True):\n",
    "        # 1. Suggest hyperparameters\n",
    "        num_class = trial.suggest_int('num_class', 2, 10)\n",
    "        max_depth = trial.suggest_int('max_depth', 2, 15)\n",
    "        n_estimators = trial.suggest_int('n_estimators', 10, 100)\n",
    "        \n",
    "        # 2. Train model\n",
    "        # 3. Predict and evaluate\n",
    "        # 4. Log to MLflow\n",
    "        \n",
    "        return accuracy\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c901c3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE: Define objective function for Optuna\n",
    "\n",
    "def objective(trial):\n",
    "    \"\"\"Objective function for hyperparameter tuning.\"\"\"\n",
    "    \n",
    "    # Step 1: Start nested MLflow run\n",
    "    \n",
    "    # Step 2: Suggest hyperparameters\n",
    "    # Hint: trial.suggest_int('num_class', 2, 10)\n",
    "    \n",
    "    # Step 3: Define XGBoost model with suggested parameters\n",
    "    \n",
    "    # Step 4: Train model\n",
    "    \n",
    "    # Step 5: Predict and calculate metrics\n",
    "    \n",
    "    # Step 6: Log parameters and metrics to MLflow\n",
    "    \n",
    "    # Step 7: Return accuracy (Optuna will maximize this)\n",
    "    \n",
    "    pass  # Remove this and write your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258fd3d0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üî¨ Task 9: Run Optimization Trials\n",
    "\n",
    "**What you need to do:**\n",
    "1. Start parent MLflow run\n",
    "2. Create Optuna study (maximize accuracy)\n",
    "3. Run 10 optimization trials\n",
    "4. Log best parameters and metrics\n",
    "5. Train final model with best parameters\n",
    "6. Save model to MLflow\n",
    "\n",
    "**üí° This takes time:** 10 trials = ~5-10 minutes depending on data size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53ef1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE: Run optimization trials\n",
    "\n",
    "# Step 1: Start parent MLflow run\n",
    "# with mlflow.start_run(experiment_id=experiment_id, run_name=\"optimize\", nested=True):\n",
    "\n",
    "    # Step 2: Create Optuna study\n",
    "    # Hint: study = optuna.create_study(direction=\"maximize\", load_if_exists=True)\n",
    "    \n",
    "    # Step 3: Run optimization\n",
    "    # Hint: study.optimize(objective, n_trials=10, callbacks=[mlflow_callback])\n",
    "    \n",
    "    # Step 4: Log best parameters\n",
    "    # Hint: mlflow.log_params(study.best_params)\n",
    "    #       mlflow.log_metric(\"accuracy\", study.best_value)\n",
    "    \n",
    "    # Step 5: Set metadata tags\n",
    "    # mlflow.set_tags({\"project\": \"Turbine maintenance predictor\", ...})\n",
    "    \n",
    "    # Step 6: Train final model with best parameters\n",
    "    # Hint: model = XGBClassifier(**study.best_params).fit(X_train, y_train)\n",
    "    \n",
    "    # Step 7: Calculate final metrics\n",
    "    \n",
    "    # Step 8: Log final model\n",
    "    # mlflow.xgboost.log_model(\n",
    "    #     xgb_model=model,\n",
    "    #     artifact_path=\"xgb_model\",\n",
    "    #     input_example=input_example,\n",
    "    #     signature=signature\n",
    "    # )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c774c7b6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìä Task 10: Compare Optimization Results\n",
    "\n",
    "**What you need to do:**\n",
    "1. Search for all runs with logged models\n",
    "2. Compare best pre-optimization vs post-optimization accuracy\n",
    "3. Determine if optimization improved performance\n",
    "\n",
    "**üí° Hint:** Filter for runs with models: `filter_string=\"tags.'mlflow.log-model.history' !=''\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c83df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE: Search for runs with logged models\n",
    "# Hint: mlflow.search_runs(\n",
    "#           filter_string=\"tags.'mlflow.log-model.history' !=''\",\n",
    "#           order_by=['metrics.accuracy DESC']\n",
    "#       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ba1ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE: Compare pre vs post optimization\n",
    "# Get best_run from Task 6 (before optimization)\n",
    "# Get best_opt_run from optimization results\n",
    "# Compare accuracies\n",
    "\n",
    "# Hint:\n",
    "# print(f\"Best previous accuracy: {best_run['metrics.accuracy']}\")\n",
    "# best_opt_run = mlflow.search_runs(...).iloc[0]\n",
    "# print(f\"Best optimized accuracy: {best_opt_run['metrics.accuracy']}\")\n",
    "# if best_opt_run['metrics.accuracy'] > best_run['metrics.accuracy']:\n",
    "#     print(\"Optimization improved accuracy!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6881910",
   "metadata": {},
   "source": [
    "### ü§î Reflection Questions:\n",
    "\n",
    "**1. Did hyperparameter tuning improve accuracy?**\n",
    "```\n",
    "[Your answer: Yes/No, by how much?]\n",
    "```\n",
    "\n",
    "**2. What were the optimal hyperparameters?**\n",
    "```\n",
    "[Your answer: num_class, max_depth, n_estimators]\n",
    "```\n",
    "\n",
    "**3. Would you run more trials (20, 50, 100) in production?**\n",
    "```\n",
    "[Your answer: Consider time vs accuracy tradeoff]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1071b84d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üß™ Task 11: Test Best Model\n",
    "\n",
    "**What you need to do:**\n",
    "1. Get best model URI from MLflow\n",
    "2. Load model from artifact store\n",
    "3. Make predictions on test data\n",
    "4. Verify model works correctly\n",
    "\n",
    "**üí° This validates the model before registering to Unity Catalog**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306e7134",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE: Get best model URI\n",
    "# Hint: Construct URI based on run_id and artifact_path\n",
    "# Format: f\"runs:/{run_id}/xgb_model\" or f\"runs:/{run_id}/logreg_model\"\n",
    "\n",
    "# best_model_uri = f\"runs:/{best_opt_run_id}/...\"\n",
    "# model_name = \"turbine_maintenance\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409ab9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE: Load model from MLflow\n",
    "# Hint: loaded_model = mlflow.pyfunc.load_model(best_model_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10453c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE: Make predictions with loaded model\n",
    "# Hint: loaded_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a3b39d",
   "metadata": {},
   "source": [
    "### ‚úÖ Success Criteria:\n",
    "- Model loads successfully\n",
    "- Predictions match previous results\n",
    "- No errors during inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3c4df4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üèÜ Task 12: Register Model to Unity Catalog\n",
    "\n",
    "**What you need to do:**\n",
    "1. Register best model to Unity Catalog\n",
    "2. Set model alias to `@prod` (production-ready)\n",
    "\n",
    "**üí° Why Unity Catalog?**\n",
    "- **Governance:** Track who uses the model\n",
    "- **Versioning:** Multiple model versions with aliases\n",
    "- **Access control:** Fine-grained permissions\n",
    "- **Lineage:** See which data created the model\n",
    "\n",
    "**Hints:**\n",
    "- Use `mlflow.register_model()` with full UC path: `f\"{catalog}.{db}.{model_name}\"`\n",
    "- Set alias: `MlflowClient().set_registered_model_alias()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e767795",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE: Register model to Unity Catalog\n",
    "# Hint: latest_model = mlflow.register_model(\n",
    "#           best_model_uri,\n",
    "#           f\"{catalog}.{db}.{model_name}\"\n",
    "#       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a222a361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE: Set @prod alias\n",
    "# Hint: MlflowClient().set_registered_model_alias(\n",
    "#           name=f\"{catalog}.{db}.{model_name}\",\n",
    "#           alias=\"prod\",\n",
    "#           version=latest_model.version\n",
    "#       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d64e7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE: Verify model is registered\n",
    "# Display model info from Unity Catalog\n",
    "# Hint: MlflowClient().get_registered_model(f\"{catalog}.{db}.{model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466fd6a7",
   "metadata": {},
   "source": [
    "### ‚úÖ Success Criteria:\n",
    "- Model registered in Unity Catalog\n",
    "- Model has @prod alias\n",
    "- Model version number visible\n",
    "- You can see model in Catalog Explorer UI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa2df2f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéâ Congratulations!\n",
    "\n",
    "You've completed the ML Model Training Challenge! üèÜ\n",
    "\n",
    "### What You've Accomplished:\n",
    "\n",
    "‚úÖ **Data Preparation** - Loaded and split training data  \n",
    "‚úÖ **Baseline Model** - Trained Logistic Regression (Level 1)  \n",
    "‚úÖ **Advanced Model** - Trained XGBoost Classifier (Level 2)  \n",
    "‚úÖ **Model Comparison** - Evaluated multiple algorithms  \n",
    "‚úÖ **Hyperparameter Optimization** - Used Optuna for tuning (Level 3)  \n",
    "‚úÖ **MLflow Tracking** - Logged all experiments  \n",
    "‚úÖ **Unity Catalog Registry** - Registered production model  \n",
    "\n",
    "### Key Results:\n",
    "\n",
    "**Best Model Accuracy:** [Your result]  \n",
    "**Model Type:** [Logistic Regression / XGBoost]  \n",
    "**Production Ready:** [Yes/No]  \n",
    "\n",
    "---\n",
    "\n",
    "## üìà Next Steps:\n",
    "\n",
    "**üìò Notebook 04.3: Model Deployment**\n",
    "- Deploy model for batch inference (Spark UDF)\n",
    "- Create REST API endpoint for real-time predictions\n",
    "- Monitor model performance in production\n",
    "\n",
    "---\n",
    "\n",
    "## üí≠ Final Reflection\n",
    "\n",
    "**1. What was the most challenging part of model training?**\n",
    "```\n",
    "[Your answer]\n",
    "```\n",
    "\n",
    "**2. How would you improve the model further?**\n",
    "Options:\n",
    "- Add more features (from 04.1 bonus exercises)\n",
    "- Try more algorithms (Random Forest, Neural Networks)\n",
    "- Handle class imbalance (SMOTE, class weights)\n",
    "- Collect more training data\n",
    "```\n",
    "[Your answer]\n",
    "```\n",
    "\n",
    "**3. What did you learn about MLflow and experiment tracking?**\n",
    "```\n",
    "[Your answer]\n",
    "```\n",
    "\n",
    "**4. How would you explain this model to a business stakeholder?**\n",
    "```\n",
    "[Your answer: Focus on business value, not technical details]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Great work!** üöÄ Ready to deploy in 04.3!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
