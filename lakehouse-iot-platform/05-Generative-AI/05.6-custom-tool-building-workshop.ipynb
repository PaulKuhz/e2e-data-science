{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87af52e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "%pip install databricks-sdk==0.40.0 databricks-feature-engineering==0.8.0 --quiet\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2618eba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize\n",
    "%run ../_resources/00-setup $reset_all_data=false\n",
    "\n",
    "import sys\n",
    "sys.path.append('../_resources')\n",
    "from gamification_framework import (\n",
    "    init_learner,\n",
    "    display_challenge_intro,\n",
    "    display_challenge_success,\n",
    "    ChallengeValidator\n",
    ")\n",
    "\n",
    "learner = init_learner()\n",
    "display_challenge_intro(\n",
    "    challenge_name=\"Custom Tool Building Workshop\",\n",
    "    difficulty=\"Intermediate\",\n",
    "    points=200,\n",
    "    description=\"Build production-ready AI tools from scratch. Learn design patterns, error handling, and testing strategies for agent tools.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc880e57",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìö Lesson 1: Tool Anatomy\n",
    "\n",
    "Every great AI tool has three components:\n",
    "\n",
    "### 1Ô∏è‚É£ **Clear Purpose**\n",
    "- What specific problem does it solve?\n",
    "- When should an agent use it?\n",
    "\n",
    "### 2Ô∏è‚É£ **Well-Defined Interface**  \n",
    "- What inputs does it need?\n",
    "- What outputs does it produce?\n",
    "- What are the edge cases?\n",
    "\n",
    "### 3Ô∏è‚É£ **Robust Implementation**\n",
    "- Error handling\n",
    "- Input validation\n",
    "- Clear error messages\n",
    "\n",
    "Let's see an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389473da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: A well-designed tool\n",
    "\n",
    "# First, create sample data table\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {catalog}.{schema}.technician_availability (\n",
    "    technician_id STRING,\n",
    "    name STRING,\n",
    "    skill_level STRING,\n",
    "    available_date DATE,\n",
    "    location STRING,\n",
    "    hourly_rate DOUBLE\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "# Insert sample data\n",
    "spark.sql(f\"\"\"\n",
    "INSERT OVERWRITE {catalog}.{schema}.technician_availability VALUES\n",
    "('TECH-001', 'Maria Garcia', 'Senior', current_date(), 'North Region', 85.00),\n",
    "('TECH-002', 'John Smith', 'Junior', current_date(), 'North Region', 45.00),\n",
    "('TECH-003', 'Sarah Chen', 'Senior', current_date() + 1, 'South Region', 90.00),\n",
    "('TECH-004', 'Ahmed Hassan', 'Mid-Level', current_date(), 'North Region', 65.00),\n",
    "('TECH-005', 'Lisa Anderson', 'Senior', current_date() + 2, 'Central Region', 88.00)\n",
    "\"\"\")\n",
    "\n",
    "print(\"‚úÖ Sample data created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af38e0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now create the UC Function (AI Tool)\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE FUNCTION {catalog}.{schema}.find_available_technicians(\n",
    "    skill_level STRING COMMENT 'Required skill level: Senior, Mid-Level, or Junior',\n",
    "    location STRING COMMENT 'Preferred location region',\n",
    "    date_needed DATE COMMENT 'Date when technician is needed'\n",
    ")\n",
    "RETURNS STRING\n",
    "COMMENT 'Finds available technicians matching the criteria. Returns formatted list with names, rates, and availability.'\n",
    "RETURN (\n",
    "    SELECT \n",
    "        CASE \n",
    "            WHEN COUNT(*) = 0 THEN \n",
    "                'No technicians available matching criteria. Consider expanding search to nearby regions or different dates.'\n",
    "            ELSE\n",
    "                CONCAT(\n",
    "                    'Found ', CAST(COUNT(*) AS STRING), ' available technician(s):\\n',\n",
    "                    CONCAT_WS('\\n', \n",
    "                        COLLECT_LIST(\n",
    "                            CONCAT(\n",
    "                                '‚Ä¢ ', name, ' (', skill_level, ') - $', \n",
    "                                CAST(hourly_rate AS STRING), '/hr - Available: ', \n",
    "                                CAST(available_date AS STRING), ' - Location: ', location\n",
    "                            )\n",
    "                        )\n",
    "                    )\n",
    "                )\n",
    "        END\n",
    "    FROM {catalog}.{schema}.technician_availability\n",
    "    WHERE skill_level = find_available_technicians.skill_level\n",
    "        AND location = find_available_technicians.location  \n",
    "        AND available_date <= find_available_technicians.date_needed\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "print(\"‚úÖ Tool created: find_available_technicians\")\n",
    "print(\"\\nüìù Tool Features:\")\n",
    "print(\"  ‚úì Clear parameter descriptions (COMMENT)\")\n",
    "print(\"  ‚úì Handles zero results gracefully\")\n",
    "print(\"  ‚úì Returns formatted, human-readable output\")\n",
    "print(\"  ‚úì Includes helpful context (rates, dates, locations)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbdb53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the tool\n",
    "result = spark.sql(f\"\"\"\n",
    "SELECT {catalog}.{schema}.find_available_technicians(\n",
    "    'Senior',\n",
    "    'North Region', \n",
    "    current_date()\n",
    ") as result\n",
    "\"\"\").collect()[0]['result']\n",
    "\n",
    "print(\"üß™ Test Result:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c923152d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéØ Challenge 1: Build a Cost Estimation Tool\n",
    "\n",
    "**Your Task:** Create a UC Function that estimates maintenance costs.\n",
    "\n",
    "**Requirements:**\n",
    "\n",
    "1. **Function Name**: `estimate_maintenance_cost`\n",
    "2. **Parameters**:\n",
    "   - `issue_type` (STRING): Type of issue (bearing, blade, gearbox, electrical, sensor)\n",
    "   - `severity` (STRING): Low, Medium, High, Critical\n",
    "   - `parts_needed` (INT): Number of parts to replace\n",
    "3. **Business Logic**:\n",
    "   - Base costs: bearing=$5000, blade=$25000, gearbox=$50000, electrical=$8000, sensor=$2000\n",
    "   - Severity multiplier: Low=1.0x, Medium=1.5x, High=2.0x, Critical=3.0x\n",
    "   - Additional parts: +20% per extra part beyond the first\n",
    "   - Labor: 15% of parts cost\n",
    "4. **Output**: Formatted cost breakdown with total\n",
    "5. **Error Handling**: Handle invalid inputs gracefully"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227e7004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üí™ YOUR TURN - Build the cost estimation tool\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE FUNCTION {catalog}.{schema}.estimate_maintenance_cost(\n",
    "    -- TODO: Define your parameters with COMMENT descriptions\n",
    ")\n",
    "RETURNS STRING\n",
    "COMMENT 'TODO: Add a clear description of what this tool does'\n",
    "RETURN (\n",
    "    -- TODO: Implement the cost calculation logic\n",
    "    -- \n",
    "    -- Hints:\n",
    "    -- - Use CASE statements for base costs and multipliers\n",
    "    -- - Calculate parts cost, labor cost, and total\n",
    "    -- - Format output as a readable breakdown\n",
    "    -- - Handle edge cases (invalid issue type, negative parts, etc.)\n",
    "    -- \n",
    "    -- Example output format:\n",
    "    -- üí∞ Maintenance Cost Estimate:\n",
    "    -- Issue Type: Gearbox (High Severity)\n",
    "    -- Base Cost: $50,000\n",
    "    -- Severity Multiplier: 2.0x\n",
    "    -- Parts Cost: $100,000 (2 parts)\n",
    "    -- Labor Cost: $15,000 (15%)\n",
    "    -- TOTAL ESTIMATED COST: $115,000\n",
    "    \n",
    "    SELECT 'TODO: Implement cost calculation'\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "print(\"‚úÖ Your function created - now test it!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4629704b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your tool with different scenarios\n",
    "\n",
    "test_cases = [\n",
    "    (\"gearbox\", \"High\", 2),\n",
    "    (\"sensor\", \"Low\", 1),\n",
    "    (\"blade\", \"Critical\", 3),\n",
    "    (\"invalid_type\", \"Medium\", 1),  # Test error handling\n",
    "]\n",
    "\n",
    "print(\"üß™ Testing your tool:\\n\")\n",
    "for issue_type, severity, parts in test_cases:\n",
    "    try:\n",
    "        result = spark.sql(f\"\"\"\n",
    "        SELECT {catalog}.{schema}.estimate_maintenance_cost(\n",
    "            '{issue_type}', '{severity}', {parts}\n",
    "        ) as result\n",
    "        \"\"\").collect()[0]['result']\n",
    "        print(f\"Test: {issue_type}, {severity}, {parts} parts\")\n",
    "        print(result)\n",
    "        print(\"-\" * 60)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {str(e)}\")\n",
    "        print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3191fff1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìö Lesson 2: Python-Based Tools\n",
    "\n",
    "Some tools need more complex logic. Let's build one in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af5e785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Python UDF for complex scheduling logic\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "\n",
    "@udf(returnType=StringType())\n",
    "def schedule_maintenance_window(turbine_id: str, priority: str, estimated_hours: int) -> str:\n",
    "    \"\"\"\n",
    "    Intelligently schedules maintenance based on priority and capacity.\n",
    "    \n",
    "    Args:\n",
    "        turbine_id: Unique turbine identifier\n",
    "        priority: CRITICAL, HIGH, MEDIUM, LOW\n",
    "        estimated_hours: Expected maintenance duration\n",
    "        \n",
    "    Returns:\n",
    "        JSON string with recommended schedule\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Priority-based scheduling\n",
    "        priority_days = {\n",
    "            'CRITICAL': 0,  # Immediate\n",
    "            'HIGH': 1,\n",
    "            'MEDIUM': 3,\n",
    "            'LOW': 7\n",
    "        }\n",
    "        \n",
    "        days_to_add = priority_days.get(priority.upper(), 7)\n",
    "        start_date = datetime.now() + timedelta(days=days_to_add)\n",
    "        \n",
    "        # Optimize for minimal grid impact (schedule at night for non-critical)\n",
    "        if priority.upper() != 'CRITICAL':\n",
    "            start_time = \"22:00\"  # 10 PM\n",
    "        else:\n",
    "            start_time = \"ASAP\"  \n",
    "        \n",
    "        # Calculate end time\n",
    "        if start_time != \"ASAP\":\n",
    "            end_datetime = start_date + timedelta(hours=estimated_hours)\n",
    "            end_time = end_datetime.strftime(\"%H:%M\")\n",
    "        else:\n",
    "            end_time = f\"+{estimated_hours}hrs\"\n",
    "        \n",
    "        result = {\n",
    "            \"turbine_id\": turbine_id,\n",
    "            \"priority\": priority,\n",
    "            \"scheduled_date\": start_date.strftime(\"%Y-%m-%d\"),\n",
    "            \"start_time\": start_time,\n",
    "            \"estimated_duration_hours\": estimated_hours,\n",
    "            \"end_time\": end_time,\n",
    "            \"notes\": f\"{'EMERGENCY - Immediate action required' if priority == 'CRITICAL' else 'Scheduled during off-peak hours'}\"\n",
    "        }\n",
    "        \n",
    "        return json.dumps(result, indent=2)\n",
    "        \n",
    "    except Exception as e:\n",
    "        return json.dumps({\"error\": f\"Scheduling failed: {str(e)}\"})\n",
    "\n",
    "# Register as SQL function\n",
    "spark.udf.register(\"schedule_maintenance_window\", schedule_maintenance_window)\n",
    "\n",
    "print(\"‚úÖ Python-based tool registered\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a901feef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the Python tool\n",
    "result = spark.sql(\"\"\"\n",
    "SELECT schedule_maintenance_window('WT-042', 'CRITICAL', 6) as schedule\n",
    "\"\"\").collect()[0]['schedule']\n",
    "\n",
    "print(\"üìÖ Scheduling Result:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63ee4a1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéØ Challenge 2: Build a Smart Diagnostic Tool\n",
    "\n",
    "**Your Task:** Create a Python UDF that analyzes sensor patterns and suggests root causes.\n",
    "\n",
    "**Requirements:**\n",
    "\n",
    "1. **Function Name**: `diagnose_turbine_issue`\n",
    "2. **Parameters**:\n",
    "   - `vibration_level` (DOUBLE): mm/s\n",
    "   - `temperature` (DOUBLE): Celsius\n",
    "   - `power_output` (DOUBLE): MW\n",
    "   - `rated_capacity` (DOUBLE): MW\n",
    "3. **Diagnostic Logic**:\n",
    "   - High vibration + Normal temp = Bearing issue\n",
    "   - High temp + Low power = Gearbox overheating\n",
    "   - Normal vibration + High temp = Cooling system failure\n",
    "   - Low power + Normal other metrics = Blade pitch problem\n",
    "   - Multiple anomalies = Compound issue requiring expert\n",
    "4. **Output**: JSON with diagnosis, confidence, and recommended actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19da9ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üí™ YOUR TURN - Build the diagnostic tool\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "import json\n",
    "\n",
    "@udf(returnType=StringType())\n",
    "def diagnose_turbine_issue(\n",
    "    vibration_level: float,\n",
    "    temperature: float, \n",
    "    power_output: float,\n",
    "    rated_capacity: float\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    TODO: Add docstring explaining the function\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # TODO: Implement diagnostic logic\n",
    "        # \n",
    "        # Define thresholds:\n",
    "        # - Normal vibration: < 8 mm/s\n",
    "        # - Normal temperature: 60-75¬∞C  \n",
    "        # - Normal power: > 80% of rated capacity\n",
    "        #\n",
    "        # Check combinations and return diagnosis\n",
    "        #\n",
    "        # Return JSON format:\n",
    "        # {\n",
    "        #   \"diagnosis\": \"Issue description\",\n",
    "        #   \"root_cause\": \"Likely cause\",\n",
    "        #   \"confidence\": \"High/Medium/Low\",\n",
    "        #   \"severity\": \"Critical/High/Medium/Low\",\n",
    "        #   \"recommended_actions\": [\"action1\", \"action2\"],\n",
    "        #   \"estimated_repair_time\": \"X hours\"\n",
    "        # }\n",
    "        \n",
    "        result = {\n",
    "            \"diagnosis\": \"TODO\",\n",
    "            \"root_cause\": \"TODO\",\n",
    "            \"confidence\": \"TODO\",\n",
    "            \"severity\": \"TODO\",\n",
    "            \"recommended_actions\": [],\n",
    "            \"estimated_repair_time\": \"TODO\"\n",
    "        }\n",
    "        \n",
    "        return json.dumps(result, indent=2)\n",
    "        \n",
    "    except Exception as e:\n",
    "        return json.dumps({\"error\": f\"Diagnosis failed: {str(e)}\"})\n",
    "\n",
    "# Register the function\n",
    "spark.udf.register(\"diagnose_turbine_issue\", diagnose_turbine_issue)\n",
    "\n",
    "print(\"‚úÖ Your diagnostic tool registered\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33be929",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your diagnostic tool\n",
    "\n",
    "test_scenarios = [\n",
    "    (12.5, 70, 2.1, 2.5, \"High vibration scenario\"),\n",
    "    (7.0, 95, 1.5, 2.5, \"High temperature scenario\"),\n",
    "    (6.5, 68, 1.2, 2.5, \"Low power scenario\"),\n",
    "    (15.0, 100, 0.8, 2.5, \"Multiple anomalies\"),\n",
    "]\n",
    "\n",
    "print(\"üß™ Testing diagnostic tool:\\n\")\n",
    "for vib, temp, power, rated, description in test_scenarios:\n",
    "    result = spark.sql(f\"\"\"\n",
    "    SELECT diagnose_turbine_issue({vib}, {temp}, {power}, {rated}) as diagnosis\n",
    "    \"\"\").collect()[0]['diagnosis']\n",
    "    \n",
    "    print(f\"üìä {description}\")\n",
    "    print(f\"   Vibration: {vib} mm/s | Temp: {temp}¬∞C | Power: {power}/{rated} MW\")\n",
    "    print(result)\n",
    "    print(\"-\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423df6eb",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéØ Challenge 3: Tool Integration Test\n",
    "\n",
    "Now let's test if your tools work together as part of an agent system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc64bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate your tools are properly registered\n",
    "validator = ChallengeValidator()\n",
    "\n",
    "print(\"üîç Validating your tools...\\n\")\n",
    "\n",
    "# Check if functions exist\n",
    "tools_to_check = [\n",
    "    \"find_available_technicians\",\n",
    "    \"estimate_maintenance_cost\",\n",
    "]\n",
    "\n",
    "tools_valid = True\n",
    "for tool_name in tools_to_check:\n",
    "    is_valid = validator.validate_uc_function(catalog, schema, tool_name)\n",
    "    if not is_valid:\n",
    "        tools_valid = False\n",
    "\n",
    "if tools_valid:\n",
    "    print(\"\\n‚úÖ All tools validated successfully!\")\n",
    "    learner.complete_challenge(\"custom_tool_building\", points=200)\n",
    "    learner.award_badge(\"tool_builder\")\n",
    "    display_challenge_success(\"Custom Tool Building Workshop\", 200)\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Some tools need fixes. Review the output above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c66ad34",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéì Key Takeaways\n",
    "\n",
    "You've learned:\n",
    "\n",
    "‚úÖ **Tool Design Patterns** - Creating intuitive, AI-friendly interfaces  \n",
    "‚úÖ **SQL Functions** - Leveraging data directly in tools  \n",
    "‚úÖ **Python UDFs** - Complex logic and custom algorithms  \n",
    "‚úÖ **Error Handling** - Making tools robust and reliable  \n",
    "‚úÖ **Testing Strategies** - Validating tool behavior\n",
    "\n",
    "### üèÜ Tool Design Best Practices\n",
    "\n",
    "1. **Clear Names**: Use verb_noun pattern (e.g., `get_status`, `calculate_cost`)\n",
    "2. **Document Everything**: Use COMMENT extensively\n",
    "3. **Handle Errors**: Never let tools crash silently\n",
    "4. **Format Output**: Return human-readable results\n",
    "5. **Test Edge Cases**: Invalid inputs, null values, extreme scenarios\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Next Steps\n",
    "\n",
    "Your tools are ready! Now use them in:\n",
    "\n",
    "- **05.2-agent-creation-guide**: Build an agent using your tools\n",
    "- **05.7-multi-agent-orchestration**: Combine multiple tool-using agents\n",
    "- **05.X-real-world-scenarios**: Test tools in emergency simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2207e488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check your overall progress\n",
    "learner.display_progress()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
