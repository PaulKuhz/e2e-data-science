{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ff4abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "%pip install databricks-sdk==0.40.0 mlflow==2.18.0 --quiet\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8aefbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize progress tracking\n",
    "%run ../_resources/00-setup $reset_all_data=false\n",
    "\n",
    "import sys\n",
    "sys.path.append('../_resources')\n",
    "from gamification_framework import (\n",
    "    init_learner, \n",
    "    display_challenge_intro,\n",
    "    display_challenge_success,\n",
    "    display_badge_awarded\n",
    ")\n",
    "\n",
    "learner = init_learner()\n",
    "display_challenge_intro(\n",
    "    challenge_name=\"Prompt Engineering Basics\",\n",
    "    difficulty=\"Beginner\",\n",
    "    points=100,\n",
    "    description=\"Learn to write effective prompts that produce reliable, high-quality AI responses for turbine maintenance scenarios.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c66127",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìö Lesson 1: The Bad Prompt Problem\n",
    "\n",
    "Here's what a junior engineer wrote. Let's see why it doesn't work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e8a9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from databricks.sdk import WorkspaceClient\n",
    "import json\n",
    "\n",
    "w = WorkspaceClient()\n",
    "\n",
    "# ‚ùå BAD PROMPT EXAMPLE\n",
    "bad_prompt = \"\"\"\n",
    "Help with turbine.\n",
    "\"\"\"\n",
    "\n",
    "# Test it\n",
    "response = w.serving_endpoints.query(\n",
    "    name=\"databricks-meta-llama-3-1-70b-instruct\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": bad_prompt},\n",
    "        {\"role\": \"user\", \"content\": \"Turbine making noise\"}\n",
    "    ],\n",
    "    max_tokens=150\n",
    ")\n",
    "\n",
    "print(\"üî¥ BAD PROMPT OUTPUT:\")\n",
    "print(response.choices[0].message.content)\n",
    "print(\"\\n‚ùå Problems:\")\n",
    "print(\"  - Too vague\")\n",
    "print(\"  - No context about turbines\")\n",
    "print(\"  - No guidance on response format\")\n",
    "print(\"  - No safety considerations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7327fd",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚úÖ Lesson 2: The Good Prompt Pattern\n",
    "\n",
    "Now let's apply prompt engineering principles:\n",
    "\n",
    "### The RACE Framework\n",
    "\n",
    "- **R**ole: Define who the AI is\n",
    "- **A**ction: Specify what it should do  \n",
    "- **C**ontext: Provide relevant background\n",
    "- **E**xpectation: Describe the desired output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e014646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ GOOD PROMPT EXAMPLE\n",
    "good_prompt = \"\"\"\n",
    "You are an expert wind turbine maintenance assistant with 10+ years of field experience.\n",
    "\n",
    "Your role is to help junior technicians diagnose and resolve turbine issues quickly and safely.\n",
    "\n",
    "Context:\n",
    "- We maintain GE 2.5MW turbines at offshore wind farms\n",
    "- Technicians have basic training but need expert guidance\n",
    "- Safety is the top priority\n",
    "- Minimize downtime to maintain power generation\n",
    "\n",
    "When responding:\n",
    "1. Start with safety considerations\n",
    "2. Provide a clear diagnosis\n",
    "3. Give step-by-step troubleshooting instructions\n",
    "4. Include estimated time and parts needed\n",
    "5. Escalate to senior engineer if issue is critical\n",
    "\n",
    "Format responses as:\n",
    "üõ°Ô∏è SAFETY: [safety steps]\n",
    "üîç DIAGNOSIS: [likely cause]\n",
    "üîß STEPS: [numbered steps]\n",
    "‚è±Ô∏è ESTIMATED TIME: [time]\n",
    "üì¶ PARTS NEEDED: [list]\n",
    "‚ö†Ô∏è ESCALATE IF: [conditions]\n",
    "\"\"\"\n",
    "\n",
    "response = w.serving_endpoints.query(\n",
    "    name=\"databricks-meta-llama-3-1-70b-instruct\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": good_prompt},\n",
    "        {\"role\": \"user\", \"content\": \"Turbine making grinding noise from gearbox area\"}\n",
    "    ],\n",
    "    max_tokens=500\n",
    ")\n",
    "\n",
    "print(\"‚úÖ GOOD PROMPT OUTPUT:\")\n",
    "print(response.choices[0].message.content)\n",
    "print(\"\\n‚úÖ Much better because:\")\n",
    "print(\"  - Clear role and expertise level\")\n",
    "print(\"  - Specific context about equipment\")\n",
    "print(\"  - Structured output format\")\n",
    "print(\"  - Safety-first approach\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049e6813",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéØ Challenge 1: Write Your First Prompt\n",
    "\n",
    "**Your Task:** Create a system prompt for helping technicians interpret sensor data.\n",
    "\n",
    "**Requirements:**\n",
    "- Role: Senior Data Analyst specialized in IoT sensor analysis\n",
    "- Context: Analyzing vibration, temperature, and power output sensors\n",
    "- Output: Should identify anomalies and suggest next actions\n",
    "- Include: Confidence levels for predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9fa850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üí™ YOUR TURN - Write your prompt here!\n",
    "\n",
    "your_prompt = \"\"\"\n",
    "# TODO: Write your system prompt here using the RACE framework\n",
    "# \n",
    "# Tips:\n",
    "# - Start with a clear role definition\n",
    "# - Provide context about sensor types and normal ranges\n",
    "# - Specify output format\n",
    "# - Include confidence scoring (High/Medium/Low)\n",
    "# - Add when to alert senior engineers\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Test your prompt\n",
    "test_sensor_data = \"\"\"\n",
    "Turbine ID: WT-042\n",
    "Vibration: 12.5 mm/s (normal: <8 mm/s)\n",
    "Temperature: 85¬∞C (normal: 60-75¬∞C)\n",
    "Power Output: 2.1 MW (rated: 2.5 MW)\n",
    "\"\"\"\n",
    "\n",
    "response = w.serving_endpoints.query(\n",
    "    name=\"databricks-meta-llama-3-1-70b-instruct\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": your_prompt},\n",
    "        {\"role\": \"user\", \"content\": f\"Analyze this sensor data:\\n{test_sensor_data}\"}\n",
    "    ],\n",
    "    max_tokens=400\n",
    ")\n",
    "\n",
    "print(\"üìä YOUR PROMPT OUTPUT:\")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476b1f11",
   "metadata": {},
   "source": [
    "### ‚úÖ Self-Evaluation Checklist\n",
    "\n",
    "Did your prompt produce a response that:\n",
    "\n",
    "- [ ] Clearly identifies which sensors are abnormal?\n",
    "- [ ] Provides confidence levels?\n",
    "- [ ] Suggests specific next actions?\n",
    "- [ ] Uses the format you specified?\n",
    "- [ ] Includes escalation criteria?\n",
    "\n",
    "If you checked 4-5 boxes: **Excellent!** üéâ  \n",
    "If you checked 2-3 boxes: **Good start** - refine and rerun  \n",
    "If you checked 0-1 boxes: Review the example and try again"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f548a23",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìà Lesson 3: Few-Shot Learning\n",
    "\n",
    "Sometimes the best way to teach AI is through examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08eac443",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Few-shot prompt with examples\n",
    "few_shot_prompt = \"\"\"\n",
    "You diagnose turbine error codes and provide maintenance actions.\n",
    "\n",
    "Examples:\n",
    "\n",
    "Error Code: ERR-001\n",
    "Response: ‚ö†Ô∏è Blade Pitch Fault | üîß Recalibrate pitch motors | ‚è±Ô∏è 2 hours | üî¥ Priority: HIGH\n",
    "\n",
    "Error Code: ERR-015  \n",
    "Response: ‚ÑπÔ∏è Sensor Drift | üîß Replace temperature sensor | ‚è±Ô∏è 30 min | üü° Priority: MEDIUM\n",
    "\n",
    "Error Code: ERR-099\n",
    "Response: üö® EMERGENCY SHUTDOWN | üîß DO NOT RESTART | ‚è±Ô∏è Call Control Center | üî¥ Priority: CRITICAL\n",
    "\n",
    "Now diagnose the following error code using the same format:\n",
    "\"\"\"\n",
    "\n",
    "response = w.serving_endpoints.query(\n",
    "    name=\"databricks-meta-llama-3-1-70b-instruct\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": few_shot_prompt},\n",
    "        {\"role\": \"user\", \"content\": \"Error Code: ERR-042\"}\n",
    "    ],\n",
    "    max_tokens=200\n",
    ")\n",
    "\n",
    "print(\"üéØ Few-Shot Output:\")\n",
    "print(response.choices[0].message.content)\n",
    "print(\"\\n‚ú® Notice how it learned the format from examples!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d16ecc",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéØ Challenge 2: Create a Few-Shot Prompt\n",
    "\n",
    "**Your Task:** Create a few-shot prompt to teach AI how to prioritize maintenance work orders.\n",
    "\n",
    "**Requirements:**\n",
    "- Provide 3 example work orders with different priorities\n",
    "- Show how priority is calculated based on: safety risk, downtime impact, and part availability\n",
    "- Output format: Priority score (1-10), urgency level, and recommended schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fc4835",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üí™ YOUR TURN - Create your few-shot prompt\n",
    "\n",
    "your_few_shot_prompt = \"\"\"\n",
    "# TODO: Create a few-shot learning prompt for work order prioritization\n",
    "#\n",
    "# Include 3 examples showing:\n",
    "# Example 1: High priority (safety issue)\n",
    "# Example 2: Medium priority (performance issue)\n",
    "# Example 3: Low priority (preventive maintenance)\n",
    "#\n",
    "# Each example should show:\n",
    "# - Work Order description\n",
    "# - Your prioritization logic  \n",
    "# - Priority score (1-10)\n",
    "# - Urgency level (CRITICAL/HIGH/MEDIUM/LOW)\n",
    "# - Recommended schedule\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Test with a new work order\n",
    "test_work_order = \"\"\"\n",
    "Work Order #WO-234:\n",
    "Issue: Gearbox oil level 15% below minimum\n",
    "Current Status: Turbine operational but efficiency reduced by 8%\n",
    "Parts Available: Yes\n",
    "Safety Risk: Low (no immediate danger)\n",
    "Impact: $1,200/day revenue loss\n",
    "\"\"\"\n",
    "\n",
    "response = w.serving_endpoints.query(\n",
    "    name=\"databricks-meta-llama-3-1-70b-instruct\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": your_few_shot_prompt},\n",
    "        {\"role\": \"user\", \"content\": f\"Prioritize this work order:\\n{test_work_order}\"}\n",
    "    ],\n",
    "    max_tokens=300\n",
    ")\n",
    "\n",
    "print(\"üéØ YOUR FEW-SHOT OUTPUT:\")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3961a0a1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìä Challenge Validation & Scoring\n",
    "\n",
    "Let's evaluate your prompts and award points!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d417563f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatic quality evaluation\n",
    "def evaluate_prompt_quality(prompt_text: str, test_output: str) -> dict:\n",
    "    \"\"\"Evaluate prompt quality based on key criteria\"\"\"\n",
    "    score = 0\n",
    "    feedback = []\n",
    "    \n",
    "    # Check prompt structure\n",
    "    if len(prompt_text) > 200:\n",
    "        score += 20\n",
    "        feedback.append(\"‚úÖ Prompt has good detail (+20)\")\n",
    "    else:\n",
    "        feedback.append(\"‚ö†Ô∏è Prompt could be more detailed\")\n",
    "    \n",
    "    # Check for role definition\n",
    "    role_keywords = ['you are', 'expert', 'specialist', 'assistant']\n",
    "    if any(keyword in prompt_text.lower() for keyword in role_keywords):\n",
    "        score += 20\n",
    "        feedback.append(\"‚úÖ Clear role definition (+20)\")\n",
    "    else:\n",
    "        feedback.append(\"‚ö†Ô∏è Missing clear role definition\")\n",
    "    \n",
    "    # Check for context\n",
    "    if 'context' in prompt_text.lower() or len(prompt_text.split('\\n')) > 5:\n",
    "        score += 20\n",
    "        feedback.append(\"‚úÖ Good context provided (+20)\")\n",
    "    else:\n",
    "        feedback.append(\"‚ö†Ô∏è Could use more context\")\n",
    "    \n",
    "    # Check output has structure\n",
    "    if len(test_output) > 100 and any(char in test_output for char in [':', '-', '‚Ä¢', '1.', '2.']):\n",
    "        score += 20\n",
    "        feedback.append(\"‚úÖ Output is well-structured (+20)\")\n",
    "    else:\n",
    "        feedback.append(\"‚ö†Ô∏è Output could be more structured\")\n",
    "    \n",
    "    # Check for safety/priority keywords\n",
    "    important_concepts = ['safety', 'priority', 'critical', 'urgent', 'escalate']\n",
    "    if any(concept in prompt_text.lower() for concept in important_concepts):\n",
    "        score += 20\n",
    "        feedback.append(\"‚úÖ Includes important considerations (+20)\")\n",
    "    else:\n",
    "        feedback.append(\"‚ö†Ô∏è Missing key safety/priority considerations\")\n",
    "    \n",
    "    return {\"score\": score, \"feedback\": feedback}\n",
    "\n",
    "# Evaluate Challenge 1\n",
    "print(\"üìä CHALLENGE 1 EVALUATION\")\n",
    "print(\"=\" * 50)\n",
    "eval1 = evaluate_prompt_quality(your_prompt, response.choices[0].message.content)\n",
    "print(f\"\\nScore: {eval1['score']}/100\")\n",
    "for item in eval1['feedback']:\n",
    "    print(f\"  {item}\")\n",
    "\n",
    "# Award points and badge if earned\n",
    "if eval1['score'] >= 80:\n",
    "    learner.complete_challenge(\"prompt_engineering_basics_ch1\", points=50)\n",
    "    print(\"\\nüéâ Challenge 1 Passed! (+50 points)\")\n",
    "    \n",
    "    if eval1['score'] >= 90:\n",
    "        learner.award_badge(\"prompt_master\")\n",
    "        display_badge_awarded(\"prompt_master\")\n",
    "else:\n",
    "    print(\"\\nüí° Try improving your prompt and re-run this cell!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5945935c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéì Key Takeaways\n",
    "\n",
    "You've learned:\n",
    "\n",
    "‚úÖ The **RACE framework** for prompt engineering  \n",
    "‚úÖ How to write **specific, structured prompts**  \n",
    "‚úÖ **Few-shot learning** to teach through examples  \n",
    "‚úÖ **Quality evaluation** of prompt outputs\n",
    "\n",
    "### üìö Further Reading\n",
    "\n",
    "- [Prompt Engineering Guide](https://www.promptingguide.ai/)\n",
    "- [OpenAI Best Practices](https://platform.openai.com/docs/guides/prompt-engineering)\n",
    "- [Databricks Prompt Engineering](https://docs.databricks.com/en/generative-ai/prompt-engineering.html)\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Next Steps\n",
    "\n",
    "Ready for more? Continue to:\n",
    "\n",
    "- **05.1.2-prompt-optimization-challenge**: Advanced prompt patterns\n",
    "- **05.1.3-few-shot-learning**: Deep dive into learning from examples\n",
    "\n",
    "Or jump ahead to build your first AI agent in **05.2-agent-creation-guide**!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a078ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check your overall progress\n",
    "learner.display_progress()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
